# spark-lifelines-cox

Репозиторий демонстрирует, как обучать отдельные модели Cox Proportional Hazards (библиотека `lifelines`) на данных PySpark по каждому значению категориального столбца и выполнять масштабный инференс через векторизованные pandas UDF. Код рассчитан на Python 3.8+ и протестирован со Spark 3.2.1.

## Формула и центрирование признаков

Модель Cox оценивает частичную правдоподобность и представляет индивидуальную выживаемость как

```
S(t | x) = S0(t) ** exp((x - \bar{x})' β)
```

`lifelines` центрирует признаки (`x - \bar{x}`), поэтому средние значения тренировочных признаков (`mean_train`) сохраняются в артефактах и используются при инференсе. Базовая функция выживаемости `S0(t)` извлекается из `CoxPHFitter.baseline_survival_` и переводится на целочисленную шкалу.

## Возможности

- Обучение отдельной Cox-модели на каждый тип (`type_col`) с ограничением по числу строк на тип — прокси для контроля памяти executors.
- Детерминированный cap-sampling: при превышении `max_rows_per_type` выбираются случайные, но повторяемые строки.
- Аккуратные артефакты по типам: коэффициенты β, средние признаков, `S0(t)`.
- Продление baseline до нужного горизонта: по умолчанию хвост зацикливает последние 12 хазарда с периодом 12, опционально можно передать свою функцию.
- Масштабный инференс без `groupBy.applyInPandas` на полном датасете: векторизованные pandas UDF c broadcast артефактов.
- Обработка edge-cases: типы без событий пропускаются, неизвестные типы на инференсе возвращают `null` или ошибку.
- Сохранение всей конфигурации и артефактов в одном CSV-файле без вспомогательных манифестов.

## Установка

```bash
pip install -e .[dev]
```

**Зависимости и окружение:**

- `pyspark==3.2.1` (нужна JVM не выше Java 11).
- `pandas>=1.3,<2.0` (Spark 3.2.1 использует API с `DataFrame.iteritems`).
- `numpy>=1.22,<1.25`, `scipy>=1.10,<1.11`, `pyarrow>=4,<13`.

## Быстрый старт

```python
from pyspark.sql import SparkSession
from spark_lifelines_cox.model import SparkCoxPHByType

spark = SparkSession.builder.master("local[*]").getOrCreate()

# Подготовьте Spark DataFrame
sdf = spark.createDataFrame([
    ("A", 5, 1, 0.4),
    ("A", 3, 0, -0.2),
    ("B", 4, 1, 1.1),
], ["type", "duration", "event", "x"])

model = SparkCoxPHByType(
    type_col="type",
    duration_col="duration",
    event_col="event",
    feature_cols=["x"],
    max_rows_per_type=100_000,
)
model.fit(sdf)
model.extend_baselines(max_time=120)

pred = model.predict_survival_at_t(sdf, t=12, output_col="s12")
pred.show()
```

## Примеры

- `examples/fit_and_predict.py` — минимальный скрипт для быстрой проверки.
- `examples/tutorial.ipynb` — Jupyter-ноутбук с полным циклом обучения, инференса и сериализации в единый CSV.

### Сохранение и загрузка

```python
model.save("/tmp/cox_model.csv")
loaded = SparkCoxPHByType.load("/tmp/cox_model.csv")
cfg_model = SparkCoxPHByType.from_config("/tmp/cox_model.csv")  # только восстановление конфигурации
```

Все артефакты и конфигурация сериализуются в одном CSV-файле: первая строка хранит конфиг модели, остальные строки — данные по типам или причины пропуска. Вам больше не нужно управлять несколькими JSON-файлами.

## Параметры класса

- `max_rows_per_type`: лимит строк на тип. Используется как грубая оценка памяти: реальный объём зависит от числа признаков и количества типов.
- `min_events_per_type`: минимальное число событий, иначе тип пропускается.
- `unknown_type_policy`: поведение на инференсе для неизвестных типов (`"null"` или `"error"`).
- `baseline_estimation_method`, `penalizer`, `l1_ratio`: проксируются в `lifelines.CoxPHFitter`.
- `trim_tail_on_wide_ci`: обрезает последние таймстемпы baseline с самыми широкими доверительными интервалами (включено по умолчанию).
- `ci_width_quantile`: квантиль доверительного интервала, выше которого точки baseline считаются плохо обученными и обрезаются.

## Edge cases и политики

- **Типы без событий или с малым числом событий**: пропускаются с причиной `not_enough_events`; инференс по таким типам вернёт `null`.
- **Неизвестные типы на инференсе**: по умолчанию `null`, можно выбросить ошибку через `unknown_type_policy="error"`.
- **Пропуски в признаках**: при обучении строки с `NaN` дропаются; на инференсе возвращается `NaN` в прогнозе.
- **Продление baseline**: по умолчанию хвост зацикливает последние 12 хазарда, но можно передать функцию `extend_fn` для кастомной логики. Дополнительно можно включить/отключить обрезание плохо настроенных хвостов через `trim_tail_on_wide_ci` и настроить порог квантиля через `ci_width_quantile`.

## Логирование и прозрачность пайплайна

Основные этапы обучения, продления бейслайна, сохранения/загрузки и предсказания сопровождаются `print`-сообщениями, чтобы удобно отслеживать прогресс при запуске пайплайна в кластере или локально.

## CI

В репозитории есть GitHub Actions (`.github/workflows/ci.yml`), которые прогоняют `ruff` и `pytest`.
