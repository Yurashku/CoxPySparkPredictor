{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52f6433",
   "metadata": {},
   "source": [
    "# Cox model with Spark — end-to-end tutorial\n",
    "\n",
    "Эта тетрадка показывает полный цикл: обучение модели, немедленное сохранение артефактов в CSV, загрузку новой инстанции из этого файла и инференс по тренировочному датафрейму с фокусом на хвостах выживаемости для живых объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f52a53",
   "metadata": {},
   "source": [
    "Докстринги и комментарии в коде написаны на русском и поясняют, зачем нужны ключевые шаги: продление baseline, отбраковка типовых хвостов и broadcast артефактов для инференса. Если вы переходите к чтению исходников, ориентируйтесь на эти примечания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72c344a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T13:50:57.865750Z",
     "iopub.status.busy": "2025-12-17T13:50:57.865337Z",
     "iopub.status.idle": "2025-12-17T13:50:59.372108Z",
     "shell.execute_reply": "2025-12-17T13:50:59.369138Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from spark_lifelines_cox.model import SparkCoxPHByType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87147b03",
   "metadata": {},
   "source": [
    "## 1. Стартуем Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa22421e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T13:50:59.376884Z",
     "iopub.status.busy": "2025-12-17T13:50:59.376276Z",
     "iopub.status.idle": "2025-12-17T13:51:06.916442Z",
     "shell.execute_reply": "2025-12-17T13:51:06.913533Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/root/.pyenv/versions/3.10.19/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/17 13:51:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://87855adf9d51:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>cox_tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1ee3d06770>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('cox_tutorial').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3df28b",
   "metadata": {},
   "source": [
    "## 2. Синтетические данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72cd0d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T13:51:06.928214Z",
     "iopub.status.busy": "2025-12-17T13:51:06.927729Z",
     "iopub.status.idle": "2025-12-17T13:51:15.493843Z",
     "shell.execute_reply": "2025-12-17T13:51:15.491912Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.10.19/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:=======================================================================>                                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark df shape: 120 rows, schema: [('type', 'string'), ('duration', 'double'), ('event', 'bigint'), ('x', 'double')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+--------------------+\n",
      "|type|           duration|event|                   x|\n",
      "+----+-------------------+-----+--------------------+\n",
      "|   A|0.09784204010016541|    1|  0.2967393780454473|\n",
      "|   A|  4.843488321644426|    1| -0.2987735866784475|\n",
      "|   A|  7.196560508319127|    1|-0.04017394160294...|\n",
      "|   B|  6.892499338292188|    1| 0.20659237721603418|\n",
      "|   B|  26.75843017452848|    1|-0.08396970353317602|\n",
      "+----+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(7)\n",
    "pdf = pd.DataFrame({\n",
    "    'type': np.where(rng.random(120) > 0.5, 'A', 'B'),\n",
    "    'duration': rng.exponential(scale=6, size=120),\n",
    "    'event': rng.binomial(1, 0.75, size=120),\n",
    "    'x': rng.normal(size=120),\n",
    "})\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "print(f\"Spark df shape: {sdf.count()} rows, schema: {sdf.dtypes}\")\n",
    "sdf.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89236c88",
   "metadata": {},
   "source": [
    "## 3. Обучение, продление baseline и немедленное сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e529b5ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T13:51:15.498475Z",
     "iopub.status.busy": "2025-12-17T13:51:15.498036Z",
     "iopub.status.idle": "2025-12-17T13:51:24.268595Z",
     "shell.execute_reply": "2025-12-17T13:51:24.265816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Starting fit pipeline\n",
      "[SparkCoxPHByType] Casting columns to numeric types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Applying cap-sampling by type with max_rows_per_type= 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:=======================================================================>                                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType][A] Start training on 63 rows\n",
      "[SparkCoxPHByType][A] Trimming trailing hazards: removed 13 points\n",
      "[SparkCoxPHByType][A] Training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType][B] Start training on 57 rows\n",
      "[SparkCoxPHByType][B] Trimming trailing hazards: removed 5 points\n",
      "[SparkCoxPHByType][B] Training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Training completed. Fitted types: 2, skipped: 0\n",
      "[SparkCoxPHByType] Extending baselines to max_time=25\n",
      "[SparkCoxPHByType] Saving artifacts to /tmp/cox_tutorial.csv\n",
      "Artifacts saved to: /tmp/cox_tutorial.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------------------------+\n",
      "|type      |payload                |status                               |\n",
      "+----------+-----------------------+-------------------------------------+\n",
      "|__config__|\"{\"\"type_col\"\":\"\"type\"\"|\"\"duration_col\"\":\"\"duration\"\"        |\n",
      "|A         |\"{\"\"type_value\"\":\"\"A\"\" |\"\"beta\"\":{\"\"x\"\":-0.17203093802722835}|\n",
      "|B         |\"{\"\"type_value\"\":\"\"B\"\" |\"\"beta\"\":{\"\"x\"\":-0.2812265147202562} |\n",
      "+----------+-----------------------+-------------------------------------+\n",
      "\n",
      "Trained types: ['A', 'B']\n",
      "Tail survival sample (last 3 points) for A [0.12857223387976638, 0.12857223387976638, 0.09340399426697549]\n"
     ]
    }
   ],
   "source": [
    "model = SparkCoxPHByType(\n",
    "    type_col='type',\n",
    "    duration_col='duration',\n",
    "    event_col='event',\n",
    "    feature_cols=['x'],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "model.fit(sdf)\n",
    "model.extend_baselines(max_time=25)\n",
    "\n",
    "csv_path = '/tmp/cox_tutorial.csv'\n",
    "model.save(csv_path)\n",
    "print(f\"Artifacts saved to: {csv_path}\")\n",
    "\n",
    "spark.read.csv(csv_path, header=True).show(4, truncate=False)\n",
    "print(\"Trained types:\", sorted(model.artifacts.keys()))\n",
    "first_type = sorted(model.artifacts.keys())[0]\n",
    "print(\n",
    "    \"Tail survival sample (last 3 points) for\",\n",
    "    first_type,\n",
    "    model.artifacts[first_type].baseline_survival[-3:],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b5eb1",
   "metadata": {},
   "source": [
    "## 4. Создание новой модели из CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d032efe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T13:51:24.273933Z",
     "iopub.status.busy": "2025-12-17T13:51:24.273505Z",
     "iopub.status.idle": "2025-12-17T13:51:24.302363Z",
     "shell.execute_reply": "2025-12-17T13:51:24.299340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Loading artifacts from /tmp/cox_tutorial.csv\n",
      "Loaded feature_cols: ['x']\n",
      "Loaded types: ['A', 'B']\n",
      "[SparkCoxPHByType] Loading configuration only from /tmp/cox_tutorial.csv\n",
      "Config-only model seed: 42\n",
      "Config-only feature cols: ['x']\n"
     ]
    }
   ],
   "source": [
    "loaded = SparkCoxPHByType.load(csv_path)\n",
    "print(\"Loaded feature_cols:\", loaded.feature_cols)\n",
    "print(\"Loaded types:\", sorted(loaded.artifacts.keys()))\n",
    "\n",
    "config_only = SparkCoxPHByType.from_config(csv_path)\n",
    "print(\"Config-only model seed:\", config_only.seed)\n",
    "print(\"Config-only feature cols:\", config_only.feature_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e7503",
   "metadata": {},
   "source": [
    "## 5. Прогноз по тренировочному датасету с фильтрацией живых объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fbbe154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T13:51:24.308778Z",
     "iopub.status.busy": "2025-12-17T13:51:24.308206Z",
     "iopub.status.idle": "2025-12-17T13:51:31.086496Z",
     "shell.execute_reply": "2025-12-17T13:51:31.084374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Building survival UDF for prediction\n",
      "[SparkCoxPHByType] Starting prediction DataFrame transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+-------------------+\n",
      "|type|           duration|event|         s10_loaded|\n",
      "+----+-------------------+-----+-------------------+\n",
      "|   A|0.09784204010016541|    1| 0.2979355068468683|\n",
      "|   A|  4.843488321644426|    1| 0.2614517304680958|\n",
      "|   A|  7.196560508319127|    1|0.27716733019341344|\n",
      "|   B|  6.892499338292188|    1|0.27203315658738986|\n",
      "|   B|  26.75843017452848|    1|0.24349084121407844|\n",
      "+----+-------------------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alive rows for tail prediction: 32\n",
      "[SparkCoxPHByType] Building survival UDF for prediction\n",
      "[SparkCoxPHByType] Starting prediction DataFrame transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:===================================>                                                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-----+-------------------+\n",
      "|type|          duration|event|          s25_alive|\n",
      "+----+------------------+-----+-------------------+\n",
      "|   B|1.1441997185120298|    0|0.24921195901995766|\n",
      "|   A| 8.159188001130543|    0|0.22773824666021145|\n",
      "|   B|2.0535088772531407|    0|0.19995123263260595|\n",
      "|   B| 21.66735633435271|    0|0.17671227532023756|\n",
      "|   A|3.8066704037950903|    0|0.14561671854066843|\n",
      "+----+------------------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:======================================================================>                                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                                                                \r"
     ]
    }
   ],
   "source": [
    "full_pred = loaded.predict_survival_at_t(sdf, t=10, output_col='s10_loaded')\n",
    "full_pred.select('type', 'duration', 'event', 's10_loaded').show(5)\n",
    "\n",
    "alive_sdf = sdf.filter(col('event') == 0)\n",
    "print(f\"Alive rows for tail prediction: {alive_sdf.count()}\")\n",
    "\n",
    "tail_pred = loaded.predict_survival_at_t(alive_sdf, t=25, output_col='s25_alive')\n",
    "# хвостовая вероятность выживания после продления baseline\n",
    "(tail_pred\n",
    "    .select('type', 'duration', 'event', 's25_alive')\n",
    "    .orderBy(col('s25_alive').desc())\n",
    "    .show(5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a8554",
   "metadata": {},
   "source": [
    "## 6. Останавливаем Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0125c587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T13:51:31.091348Z",
     "iopub.status.busy": "2025-12-17T13:51:31.090745Z",
     "iopub.status.idle": "2025-12-17T13:51:32.036474Z",
     "shell.execute_reply": "2025-12-17T13:51:32.035328Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
