{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d8e8a2",
   "metadata": {},
   "source": [
    "# Cox model with Spark — end-to-end tutorial\n",
    "\n",
    "Эта тетрадка показывает минимальный рабочий цикл: обучение, продление baseline, инференс, сохранение в единый CSV и создание модели только из конфигурации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c82ca0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:13:16.218281Z",
     "iopub.status.busy": "2025-12-17T12:13:16.217986Z",
     "iopub.status.idle": "2025-12-17T12:13:17.276664Z",
     "shell.execute_reply": "2025-12-17T12:13:17.275298Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from spark_lifelines_cox.model import SparkCoxPHByType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7595b49",
   "metadata": {},
   "source": [
    "## 1. Стартуем Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fcf8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:13:17.280656Z",
     "iopub.status.busy": "2025-12-17T12:13:17.280187Z",
     "iopub.status.idle": "2025-12-17T12:13:22.099620Z",
     "shell.execute_reply": "2025-12-17T12:13:22.096639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/root/.pyenv/versions/3.10.19/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/17 12:13:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9325a9c364c0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>cox_tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdef3dbb640>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('cox_tutorial').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8eb1a3",
   "metadata": {},
   "source": [
    "## 2. Синтетические данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2744eee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:13:22.108824Z",
     "iopub.status.busy": "2025-12-17T12:13:22.108456Z",
     "iopub.status.idle": "2025-12-17T12:13:26.662343Z",
     "shell.execute_reply": "2025-12-17T12:13:26.659658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.10.19/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+--------------------+\n",
      "|type|           duration|event|                   x|\n",
      "+----+-------------------+-----+--------------------+\n",
      "|   A|0.09784204010016541|    1|  0.2967393780454473|\n",
      "|   A|  4.843488321644426|    1| -0.2987735866784475|\n",
      "|   A|  7.196560508319127|    1|-0.04017394160294...|\n",
      "|   B|  6.892499338292188|    1| 0.20659237721603418|\n",
      "|   B|  26.75843017452848|    1|-0.08396970353317602|\n",
      "+----+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                                                                \r"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(7)\n",
    "pdf = pd.DataFrame({\n",
    "    'type': np.where(rng.random(120) > 0.5, 'A', 'B'),\n",
    "    'duration': rng.exponential(scale=6, size=120),\n",
    "    'event': rng.binomial(1, 0.75, size=120),\n",
    "    'x': rng.normal(size=120),\n",
    "})\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579104d",
   "metadata": {},
   "source": [
    "## 3. Обучение и продление baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed8a400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:13:26.668540Z",
     "iopub.status.busy": "2025-12-17T12:13:26.668147Z",
     "iopub.status.idle": "2025-12-17T12:13:31.781363Z",
     "shell.execute_reply": "2025-12-17T12:13:31.780471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Starting fit pipeline\n",
      "[SparkCoxPHByType] Casting columns to numeric types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Applying cap-sampling by type with max_rows_per_type= 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                                                                          (0 + 3) / 3]\r",
      "\r",
      "[Stage 1:==============>                            (1 + 2) / 3][Stage 2:>                                          (0 + 2) / 3]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                                                                          (0 + 3) / 3]\r",
      "\r",
      "[Stage 2:===================================>                                                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType][A] Start training on 63 rows\n",
      "[SparkCoxPHByType][A] Trimming trailing hazards: removed 13 points\n",
      "[SparkCoxPHByType][A] Training finished\n",
      "[SparkCoxPHByType][B] Start training on 57 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Training completed. Fitted types: 2, skipped: 0\n",
      "[SparkCoxPHByType] Extending baselines to max_time=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType][B] Trimming trailing hazards: removed 5 points\n",
      "[SparkCoxPHByType][B] Training finished\n",
      "\r",
      "                                                                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A': TypeArtifacts(type_value='A', beta={'x': -0.17203093802722835}, mean_={'x': -0.20381435735001205}, baseline_survival=[1.0, 0.7487158764901778, 0.7007297058019774, 0.6139382135452179, 0.595255199804578, 0.49949025889663584, 0.46061099374514286, 0.40020118113866915, 0.33613803171349604, 0.3133087681088635, 0.26719704610433026, 0.2174604546538099, 0.2174604546538099, 0.1579787054082975, 0.1579787054082975, 0.1579787054082975, 0.1579787054082975, 0.1579787054082975, 0.1579787054082975, 0.1579787054082975, 0.1579787054082975, 0.1579787054082975, 0.1579787054082975, 0.12857223387976638, 0.12857223387976638, 0.09340399426697549], baseline_ratio=[1.0, 0.7487158764901778, 0.9359087042295009, 0.87614126882572, 0.9695685765628532, 0.8391195222832464, 0.9221621153586128, 0.8688485220135701, 0.8399226378020725, 0.9320836637013129, 0.8528233911777692, 0.813858004137141, 1.0, 0.7264709607077506, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.813858004137141, 1.0, 0.7264709607077506], sample_size=63, event_count=47, fitted_at='2025-12-17T12:13:31Z', penalizer=0.0, l1_ratio=0.0, feature_cols=['x'], baseline_method='breslow'),\n",
       " 'B': TypeArtifacts(type_value='B', beta={'x': -0.2812265147202562}, mean_={'x': -0.10426917942643724}, baseline_survival=[1.0, 0.8313188775607832, 0.7062936173100479, 0.6177235846063873, 0.5479156920692132, 0.4776210441121889, 0.40457456230939837, 0.37835524931591885, 0.32577749510056325, 0.24152950761872702, 0.24152950761872702, 0.21088573993533463, 0.21088573993533463, 0.1812638957849835, 0.15083502550051256, 0.15083502550051256, 0.088791556673648, 0.088791556673648, 0.088791556673648, 0.088791556673648, 0.088791556673648, 0.088791556673648, 0.088791556673648, 0.088791556673648, 0.088791556673648, 0.088791556673648], baseline_ratio=[1.0, 0.8313188775607832, 0.8496061335482018, 0.8745988487890013, 0.8869916993995695, 0.8717053572757603, 0.847061843896408, 0.9351928780598215, 0.8610360122915746, 0.741394084156028, 1.0, 0.8731261948673951, 1.0, 0.8595360494292583, 0.83212944777174, 1.0, 0.588666699786823, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], sample_size=57, event_count=41, fitted_at='2025-12-17T12:13:31Z', penalizer=0.0, l1_ratio=0.0, feature_cols=['x'], baseline_method='breslow')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SparkCoxPHByType(\n",
    "    type_col='type',\n",
    "    duration_col='duration',\n",
    "    event_col='event',\n",
    "    feature_cols=['x'],\n",
    "    seed=42,\n",
    ")\n",
    "model.fit(sdf)\n",
    "model.extend_baselines(max_time=25)\n",
    "model.artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da70c5",
   "metadata": {},
   "source": [
    "## 4. Прогноз выживаемости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338685ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:13:31.784697Z",
     "iopub.status.busy": "2025-12-17T12:13:31.784461Z",
     "iopub.status.idle": "2025-12-17T12:13:32.303590Z",
     "shell.execute_reply": "2025-12-17T12:13:32.298494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Building survival UDF for prediction\n",
      "[SparkCoxPHByType] Starting prediction DataFrame transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|type|                s10|\n",
      "+----+-------------------+\n",
      "|   A| 0.2979355068468683|\n",
      "|   A| 0.2614517304680958|\n",
      "|   A|0.27716733019341344|\n",
      "|   B|0.27203315658738986|\n",
      "|   B|0.24349084121407844|\n",
      "+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_survival_at_t(sdf, t=10, output_col='s10')\n",
    "pred.select('type', 's10').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38ba80",
   "metadata": {},
   "source": [
    "## 5. Сохранение, загрузка и создание модели из конфига"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3abba6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:13:32.307063Z",
     "iopub.status.busy": "2025-12-17T12:13:32.306788Z",
     "iopub.status.idle": "2025-12-17T12:13:33.984105Z",
     "shell.execute_reply": "2025-12-17T12:13:33.982877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparkCoxPHByType] Saving artifacts to /tmp/cox_tutorial.csv\n",
      "[SparkCoxPHByType] Loading artifacts from /tmp/cox_tutorial.csv\n",
      "[SparkCoxPHByType] Building survival UDF for prediction\n",
      "[SparkCoxPHByType] Starting prediction DataFrame transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|type|         s10_loaded|\n",
      "+----+-------------------+\n",
      "|   A| 0.2979355068468683|\n",
      "|   A| 0.2614517304680958|\n",
      "|   A|0.27716733019341344|\n",
      "+----+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "[SparkCoxPHByType] Loading configuration only from /tmp/cox_tutorial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = '/tmp/cox_tutorial.csv'\n",
    "model.save(csv_path)\n",
    "\n",
    "loaded = SparkCoxPHByType.load(csv_path)\n",
    "re_pred = loaded.predict_survival_at_t(sdf, t=10, output_col='s10_loaded')\n",
    "re_pred.select('type', 's10_loaded').show(3)\n",
    "\n",
    "config_only = SparkCoxPHByType.from_config(csv_path)\n",
    "config_only.feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892546bd",
   "metadata": {},
   "source": [
    "## 6. Останавливаем Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b04b6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:13:33.987021Z",
     "iopub.status.busy": "2025-12-17T12:13:33.986752Z",
     "iopub.status.idle": "2025-12-17T12:13:34.972299Z",
     "shell.execute_reply": "2025-12-17T12:13:34.971149Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
